{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b67d70f3",
   "metadata": {},
   "source": [
    "# Sparse Optimization with Lasso Regression\n",
    "# Feature Selection for High-Dimensional Data\n",
    "\n",
    "## Javier FernÃ¡ndez Ramos\n",
    "\n",
    "This notebook demonstrates how to use Lasso regression for feature selection in high-dimensional datasets. Lasso (Least Absolute Shrinkage and Selection Operator) is a linear regression technique that includes a regularization term to promote sparsity in the model coefficients, effectively selecting a subset of features.\n",
    "\n",
    "The data was acquired by the public kaggle dataset \"TCGA - LUSC | Lung Cancer Gene Expression Dataset\" from https://www.kaggle.com/datasets/noepinefrin/tcga-lusc-lung-cell-squamous-carcinoma-gene-exp/data . \n",
    "\n",
    "Dataset consists of 551 patients, samples, each with 56970 differetn transcripts (expressed genes). \n",
    "\n",
    "Lung Cell Squamos Carcinoma is a cancer that occurs in lungs. Detecting and predicting it by Machine Learning is a clear challenge that could be very helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aa7e30",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e540c",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3aea35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ade79e",
   "metadata": {},
   "source": [
    "### Import needed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6276ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(551, 24763)\n",
      "(551,)\n",
      "24763\n"
     ]
    }
   ],
   "source": [
    "X = np.load(\"../data/processed/X_preprocess.npy\")\n",
    "y = np.load(\"../data/processed/y.npy\")\n",
    "gene_names = joblib.load(\"../data/processed/gene_names.pkl\")\n",
    "\n",
    "print(X.shape)   # (551, N)\n",
    "print(y.shape)          # (551,)\n",
    "print(len(gene_names))  # N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824af1f2",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "\n",
    "Before running any models, we must spit our preprocessed data into training and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd2907e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:14: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.5)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9264ee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 413\n",
      "Test samples: 138\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"Train samples:\", X_train.shape[0])\n",
    "print(\"Test samples:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80c9aa",
   "metadata": {},
   "source": [
    "### Baseline modelling\n",
    "\n",
    "Before diving into LASSO, we are going to run some baselines models to compare the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2704853",
   "metadata": {},
   "source": [
    "### Ridge Regression \n",
    "\n",
    "Now we are going to execute Ridge Regression as baseline to find the optimal penalty strength using cross-validation. \n",
    "\n",
    "Ridge applies L2 penalty, which shrinks the coefficients but does not force them to zero (does not include sparsity).\n",
    "\n",
    "##### It shows the benefit of regularization on generalization error (lowering MSE and improvinf AUC) without the added constraint of sparsity. \n",
    "\n",
    "The performance difference between Ridge Regression and Lasso will help us understand the trade off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af1b775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34cf8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Alpha (Ridge): 5179.4747\n",
      "Ridge MSE on Test Set: 0.0094\n",
      "Ridge AUC on Test Set: 1.0000\n",
      "Number of non-zero coefficients: 24763\n"
     ]
    }
   ],
   "source": [
    "# Define a range of alpha values for the cross-validation\n",
    "alphas = np.logspace(-2, 6, 50) # Log scale alpha values from 10^-2 to 10^2, from 0.01 to 100\n",
    "# The RidgeCV model will automatically search these alphas\n",
    "ridge_cv = RidgeCV(\n",
    "    alphas=alphas, # search for best performance alpha\n",
    "    cv=5, # 5-fold cross-validation. Data splitted innto 5 parts, train on 4, validate on 1, repeat 5 times\n",
    "    scoring='neg_mean_squared_error' # Optimize for lowest MSE\n",
    ")\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "ridge_y_pred = ridge_cv.predict(X_test)\n",
    "\n",
    "print(f\"Optimal Alpha (Ridge): {ridge_cv.alpha_:.4f}\")\n",
    "print(f\"Ridge MSE on Test Set: {mean_squared_error(y_test, ridge_y_pred):.4f}\")\n",
    "print(f\"Ridge AUC on Test Set: {roc_auc_score(y_test, ridge_y_pred):.4f}\") # How well the model ranks positive vs negative samples\n",
    "print(f\"Number of non-zero coefficients: {np.sum(ridge_cv.coef_ != 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2317cf8",
   "metadata": {},
   "source": [
    "We can see that finally the optimal alpha was 5179, we obtained a very small MSE and an AUC of 1... what means very good performance. This could mean two things:\n",
    "\n",
    "1. The dataset is linearly separable, allowing both Ridge and Lasso to achieve high performance.\n",
    "2. The features are highly informative, enabling effective classification even with regularization.\n",
    "3. Data leakage.\n",
    "4. Good dense baseline to compare our Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5bef85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed: genes names match the model dimensions.\n",
      "List has 24763 names, but Mmdel has 24763 coefficients.\n",
      "\n",
      "RIDGE INFLUENTIAL DISCOVERED GENES\n",
      "SPP1          0.000620\n",
      "AL035665.1    0.000580\n",
      "HBA1          0.000553\n",
      "R3HDML        0.000538\n",
      "TUBB1         0.000524\n",
      "ITLN1         0.000517\n",
      "PKHD1L1       0.000510\n",
      "MIR4530       0.000496\n",
      "LRRN4CL       0.000482\n",
      "SDS           0.000479\n",
      "dtype: float64\n",
      "\n",
      "RIDGE NEGATIVE INFLUENTIAL DISCOVERED GENES\n",
      "SPP1          0.000620\n",
      "AL035665.1    0.000580\n",
      "R3HDML        0.000538\n",
      "MIR4530       0.000496\n",
      "SDS           0.000479\n",
      "MMP9          0.000474\n",
      "MYBPH         0.000473\n",
      "MIR6774       0.000444\n",
      "DSP           0.000428\n",
      "AL117382.1    0.000427\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if len(gene_names) != len(ridge_cv.coef_):\n",
    "    print(\"Length mismatch\")\n",
    "    print(f\"List has {len(gene_names)} names, but Model has {len(ridge_cv.coef_)} coefficients.\")\n",
    "else:\n",
    "    print(f\"Sanity Check Passed: genes names match the model dimensions.\")\n",
    "    print(f\"List has {len(gene_names)} names, but Mmdel has {len(ridge_cv.coef_)} coefficients.\")\n",
    "\n",
    "    # 2. Re-attach names to the coefficients\n",
    "    # We create a Pandas Series where the data is the coefs, and the index is your names\n",
    "    ridge_series = pd.Series(ridge_cv.coef_, index=gene_names)\n",
    "\n",
    "    # 3. Filter for the non-zero survivors\n",
    "    selected_genes = ridge_series[ridge_series != 0].abs().sort_values(ascending=False)\n",
    "    selected_genes.to_csv(\"final_lasso_genes.csv\")\n",
    "\n",
    "    print(\"\\nRIDGE INFLUENTIAL DISCOVERED GENES\")\n",
    "    print(selected_genes.head(10))\n",
    "\n",
    "    selected_genes = ridge_series[ridge_series != 0].sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nRIDGE NEGATIVE INFLUENTIAL DISCOVERED GENES\")\n",
    "    print(selected_genes.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad45ba08",
   "metadata": {},
   "source": [
    "There is no present leakage effect of the features... then we are having a very good dense baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35129e2",
   "metadata": {},
   "source": [
    "## Lasso Challenge\n",
    "\n",
    "After proving to have a very good dense and solid baseline execution with Ridge, OLS and Dummy Regressor. We have to see if Lasso is capable to reduce the features to find the needed group of genes that can simplify the complexity of the problem adding sparsity and reducing number of features\n",
    "\n",
    "### Feature selection\n",
    "\n",
    "Now we want to see that the number of non-zero coefficients, the number of selected features are dropped from 24763 to something smaller.\n",
    "\n",
    "The performance might drop but we need to optimize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb198ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   LASSO RESULTS (Sparse Model)\n",
      "Optimal Alpha: 0.008514\n",
      "Lasso MSE: 0.0102\n",
      "Lasso AUC: 1.0000\n",
      "\n",
      "\n",
      "Original Features: 24763\n",
      "Features Selected (Non-Zero): 95\n",
      "Sparsity Achieved: 99.62% of features removed\n",
      "\n",
      "Top 5 Features selected:\n",
      "19608    0.060168\n",
      "14453    0.025671\n",
      "13138    0.016630\n",
      "21186    0.015327\n",
      "15235    0.012584\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "lasso_cv = LassoCV(\n",
    "    cv=5, \n",
    "    random_state=42, \n",
    "    max_iter=10000,  # Increased from default 1000 to ensure convergence\n",
    "    n_jobs=-1        # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Fit the Model\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "lasso_y_pred = lasso_cv.predict(X_test)\n",
    "\n",
    "# 4. Evaluate Sparsity\n",
    "# Count how many coefficients are NOT zero\n",
    "n_features = len(lasso_cv.coef_)\n",
    "n_nonzero = np.sum(lasso_cv.coef_ != 0)\n",
    "sparsity_ratio = (1 - (n_nonzero / n_features)) * 100\n",
    "\n",
    "print(\"\\n   LASSO RESULTS (Sparse Model)\")\n",
    "print(f\"Optimal Alpha: {lasso_cv.alpha_:.6f}\")\n",
    "print(f\"Lasso MSE: {mean_squared_error(y_test, lasso_y_pred):.4f}\")\n",
    "print(f\"Lasso AUC: {roc_auc_score(y_test, lasso_y_pred):.4f}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Original Features: {n_features}\")\n",
    "print(f\"Features Selected (Non-Zero): {n_nonzero}\")\n",
    "print(f\"Sparsity Achieved: {sparsity_ratio:.2f}% of features removed\")\n",
    "\n",
    "# 5. Inspect the Survivors (The features Lasso kept)\n",
    "if hasattr(X_train, 'columns'):\n",
    "    lasso_coefs = pd.Series(lasso_cv.coef_, index=X_train.columns)\n",
    "else:\n",
    "    lasso_coefs = pd.Series(lasso_cv.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c07335c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed: genes names match the model dimensions.\n",
      "List has 24763 names, but Mmdel has 24763 coefficients.\n",
      "\n",
      "TOP INFLUENTIAL DISCOVERED GENES\n",
      "RS1          0.060168\n",
      "MIR3945HG    0.025671\n",
      "LINC00702    0.016630\n",
      "SPP1         0.015327\n",
      "MYRIP        0.012584\n",
      "MYBL2        0.011655\n",
      "DSP          0.011527\n",
      "TUBB1        0.010998\n",
      "PKHD1L1      0.010713\n",
      "HBA1         0.009160\n",
      "dtype: float64\n",
      "\n",
      "TOP NEGATIVE INFLUENTIAL DISCOVERED GENES\n",
      "SPP1          0.015327\n",
      "MYBL2         0.011655\n",
      "DSP           0.011527\n",
      "HMGA1         0.008613\n",
      "AL035665.1    0.007960\n",
      "FBXO32        0.006323\n",
      "TNFRSF13C     0.005843\n",
      "DDIT4L        0.004779\n",
      "C16orf70      0.004028\n",
      "CXCL13        0.003990\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Sanity Check\n",
    "# The length of your name list MUST match the number of features in the model (24763)\n",
    "# If this fails, the list represents the wrong step in your pipeline.\n",
    "if len(gene_names) != len(lasso_cv.coef_):\n",
    "    print(\"Length mismatch\")\n",
    "    print(f\"List has {len(gene_names)} names, but Model has {len(lasso_cv.coef_)} coefficients.\")\n",
    "else:\n",
    "    print(f\"Sanity Check Passed: genes names match the model dimensions.\")\n",
    "    print(f\"List has {len(gene_names)} names, but Mmdel has {len(lasso_cv.coef_)} coefficients.\")\n",
    "\n",
    "    # 2. Re-attach names to the coefficients\n",
    "    # We create a Pandas Series where the data is the coefs, and the index is your names\n",
    "    lasso_series = pd.Series(lasso_cv.coef_, index=gene_names)\n",
    "\n",
    "    # 3. Filter for the non-zero survivors\n",
    "    selected_genes = lasso_series[lasso_series != 0].abs().sort_values(ascending=False)\n",
    "    selected_genes.to_csv(\"final_lasso_genes.csv\")\n",
    "\n",
    "    print(\"\\nTOP INFLUENTIAL DISCOVERED GENES\")\n",
    "    print(selected_genes.head(10))\n",
    "\n",
    "    selected_genes = lasso_series[lasso_series != 0].sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nTOP NEGATIVE INFLUENTIAL DISCOVERED GENES\")\n",
    "    print(selected_genes.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe66cc",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Between the most influential genes, the ones more present in ill patients, when lung cancer tumors are detected, are the following ones:\n",
    "\n",
    "- LINC00702 Gene, that is related with Meningioma, Meningeal Cell Tumor.\n",
    "\n",
    "- MYBL2 Gene, related with Myeloma. Present in patients with different cancers such as gastric, breast or neuroblastoma and cerebellera degeneration.\n",
    "\n",
    "- SPP1 Gene, related with Cholangiocarcinoma, tumors and cancer proliferations in the Liver, Skin or Nephrological diseases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
